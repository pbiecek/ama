# Snowflake AI security framework

AI security framework is a whitepaper published by Snowflake in 2024. This whitepaper approaches the security problem from the perspective of potential threats. For each of the 19 threats, there is a structured presentation of the potential (negative) effect when an adversary exploits the threat, examples of attacks, potential mitigation strategies (only a selection are presented below), what is being attacked and what risks are involved.

The following table summarises the threats and risks listed in the paper [@aisf-snowflake].



| Threat                                            | Short info                                                                                                                                                                                                                                                                      | Category     | Main Risk                                                           |  Mitigation Strategies                                                                                                                                                                                 |
| ------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------ | ------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Training Data Leakage                             | exposure of sensitive information that may be contained within the dataset(s) used to train a model                                                                                                                                                                        | Model, Data     | Confidentiality, Compliance, Legal                                  | Data Anonymization; Train-Test Split; Access Controls; Secure Data Transmission                                                                                                                            |
| Privacy                                           | privacy concerns, relating to the collection, storage, and utilization of sensitive data to train and deploy models                                                                                                                                                        | Model, Data     | Confidentiality, Compliance, Legal                                  | Differential Privacy; Data Minimization; Secure Multiparty Computation (SMC); Federated Learning;  Privacy-Preserving Evaluation                                                                           |
| Bias                                              | presence of systematic and unfair inaccuracies in predictions made by the models                                                                                                                                                                                           | Model, Data     | Integrity, Compliance, Legal                                        | Data collection and cleansing; Algorithm selection and design; Human oversight and auditing; Continuous monitoring and feedback; FairGAN                                                                   |
| Lack of Explainability / Transparency               | opacity, often referred to as the "black box" problem, presents a significant challenge across various domains, ..., explainability is crucial for building trust, mitigating bias and ensuring responsible AI development                                                 | Model, Data     | Integrity, Compliance, Legal                                        | Feature Importance; Model Explainability; Counterfactual Explanations; Adversarial Training; Model Transparency and Documentation                                                                          |
| Backdooring Models (Insider Attacks)              | manipulate models during training to introduce hidden vulnerabilities, allowing them to trigger malicious behavior during deployment                                                                                                                                       | Model, Data     | Integrity, Compliance, Legal                                        | Secure Development Practices; Data Provenance and Auditability; Model Verification and Validation; Access Control and Monitoring; Adversary Detection and Response                                         |
| Prompt Injection                                  | steer the model towards generating specific outputs, like hate speech, misinformation, or offensive content; constructing prompts that exploit the model’s internal biases; influence the model’s behavior beyond the intended task                                        | Model           | Integrity, Compliance, Legal                                        | Prompt Validation and Filtering; Adversarial Training; Context-Aware Processing; Explainable AI Techniques; Human-in-the-Loop Systems                                                                      |
| Indirect Prompt Injection                         | inject harmful instructions into external content, causing LLMs to generate unexpected and potentially harmful responses                                                                                                                                                   | Model           | Integrity, Compliance, Legal                                        | Sources Validation and Filtering; Explainable AI Techniques; Human-in-the-Loop Systems                                                                                                                     |
| Adversarial Samples                               | modification of input, often imperceptible to humans, designed to mislead an AI model                                                                                                                                                                                      | Model           | Integrity, Compliance, Legal                                        | Robust Model Training; Defensive Distillation; Input Preprocessing; Adversarial Detection Mechanisms                                                                                                       |
| Sponge Samples                                    | maliciously crafted inputs designed to significantly increase energy consumption and computational latency, ..., named for their ability to absorb resources                                                                                                               | Model           | Availability, Compliance, Legal                                     | Input Validation and Normalization; Resource Monitoring and Throttling; Adversarial Training; Hardware-Level Optimizations                                                                                 |
| Model  Stealing                                   | extracting and replicating the functionality of a target model                                                                                                                                                                                                             | Model           | Compliance, Legal                                                   | Secure Model Deployment; Watermarking and Fingerprinting; Differential Privacy Techniques; Access Control Measures                                                                                         |
| Fuzzing                                           | feeding the model with a deluge of diverse, often malformed, inputs, aiming to elicit unexpected behaviors or trigger crashes, such as: incorrect predictions, potentially impacting safety-critical systems, privacy violations, adversarial attacks, resource exhaustion | Model           | Integrity, Availability, Compliance                                 | Input Validation; Adversarial Training; Model Explainability; Fuzzing as a Defense                                                                                                                         |
| Model Inversion                                   | reverse-engineer a trained model to extract sensitive information about its training data                                                                                                                                                                                  | Model           | Confidentiality, Legal                                              | Limit access to the model and its predictions; Validate input data to prevent malicious inputs; Log all inputs and outputs for auditing; Compare predictions to ground truth data                          |
| Distributed Denial of Service on ML Model         | overwhelm ML systems, exhaust their computational resources, and render them unusable                                                                                                                                                                                      | Model           | Availability, Compliance, Legal                                     | Capacity Planning and Scaling; Resilient Infrastructure; Filtering and Traffic Anomaly Detection; Rate Limiting and Prioritization; Adaptive Defense                                                       |
| Model Poisoning                                   | infiltrate malicious data into a model’s training process, manipulating its learning and causing significant performance degradation or biased outputs                                                                                                                     | Model           | Integrity, Availability, Compliance                                 | Regularly monitor the quality and integrity of training data to detect any anomalies or malicious injections                                                                                               |
| Training Data Poisoning                           | malicious manipulation of training data to compromise the integrity and performance of AI models, induce specific behaviors in the model, to induce biases, skew model predictions, or cause targeted misclassifications                                                   | Data            | Integrity, Compliance, Legal                                        | Data Sanitization; Robust Model Training; Input Validation; Data Sanitization; Input Validation                                                                                                            |
| Multitenancy in ML Enviroments                    | maliciously access sensitive information belonging to other tenants, potentially compromising confidentiality and violating trust; impact model performance and system responsiveness across tenants                                                                       | Infrastructure  | Confidentiality, Compliance, Legal                                  | Data Encryption; Federated Learning; Differential Privacy; Secure Multi-Party Computation (SMPC); Resource Management and Monitoring                                                                       |
| Exposure of sensitive inferential inputs          | service, if not properly secured, might be a target to eavesdropping                                                                                                                                                                                                       | Infrastructure  | Confidentiality, Integrity, Legal                                   | Proper configuration of encryption in transit; Implementing authentication mechanisms; Secure communication channels                                                                                       |
| Attacks on the Infrastructure Hosting ML Services | infrastructure components may be susceptible to a range of technical attack vectors, potentially jeopardizing the entire ML system and exposing sensitive information                                                                                                      | Infrastructure  | Confidentiality, Integrity, Availability, Compliance, Legal         | Patch Management and Vulnerability Scanning; Least Privilege and Access Control; Data Encryption; Security Testing and Penetration Testing; Continuous Monitoring and Logging; Security Awareness Training |
| Self-Hosted OSS LLMS Security                     | hosting and utilizing third-party, pretrained models poses security challenges                                                                                                                                                                                             | General         |  Confidentiality, Integrity, Availability, Compliance, Legal       | Supply Chain Security; Cryptographic Signing; Model Testing; Malware Scanning; Secure Deployment and Monitoring                                                                                            |

