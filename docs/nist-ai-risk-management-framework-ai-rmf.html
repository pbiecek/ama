<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 NIST AI Risk Management Framework (AI RMF) | Adversarial Model Analysis</title>
  <meta name="description" content="Red Team Notes" />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 NIST AI Risk Management Framework (AI RMF) | Adversarial Model Analysis" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/images/cover.png" />
  <meta property="og:description" content="Red Team Notes" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 NIST AI Risk Management Framework (AI RMF) | Adversarial Model Analysis" />
  
  <meta name="twitter:description" content="Red Team Notes" />
  <meta name="twitter:image" content="/images/cover.png" />

<meta name="author" content="Przemysław Biecek" />


<meta name="date" content="2024-05-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="open-worldwide-application-security-project-owasp.html"/>
<link rel="next" href="adversarial-robustness-toolbox-art.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><h3>Adversarial Model Analysis</h3> RedTeam Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html"><i class="fa fa-check"></i><b>1</b> The Two XAI Cultures</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#motivation"><i class="fa fa-check"></i><b>1.1</b> Motivation</a></li>
<li class="chapter" data-level="1.2" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#introduction-1"><i class="fa fa-check"></i><b>1.2</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#user-oriented-explanations"><i class="fa fa-check"></i><b>1.2.1</b> User Oriented Explanations</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#the-developer-oriented-explanations"><i class="fa fa-check"></i><b>1.2.2</b> The Developer Oriented Explanations</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#challenges"><i class="fa fa-check"></i><b>1.3</b> Challenges</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#achilles-heels"><i class="fa fa-check"></i><b>1.3.1</b> Achilles’ heels</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#rashomon-perspectives"><i class="fa fa-check"></i><b>1.3.2</b> Rashomon perspectives</a></li>
<li class="chapter" data-level="1.3.3" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#champion-challenger-analysis"><i class="fa fa-check"></i><b>1.3.3</b> Champion Challenger analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="model-evaluation-levels.html"><a href="model-evaluation-levels.html"><i class="fa fa-check"></i><b>2</b> Model Evaluation Levels</a></li>
<li class="chapter" data-level="3" data-path="mitre-attck.html"><a href="mitre-attck.html"><i class="fa fa-check"></i><b>3</b> MITRE ATT&amp;CK</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mitre-attck.html"><a href="mitre-attck.html#tactics"><i class="fa fa-check"></i><b>3.1</b> Tactics</a></li>
<li class="chapter" data-level="3.2" data-path="mitre-attck.html"><a href="mitre-attck.html#techniques"><i class="fa fa-check"></i><b>3.2</b> Techniques</a></li>
<li class="chapter" data-level="3.3" data-path="mitre-attck.html"><a href="mitre-attck.html#procedures"><i class="fa fa-check"></i><b>3.3</b> Procedures</a></li>
<li class="chapter" data-level="3.4" data-path="mitre-attck.html"><a href="mitre-attck.html#attck-for-ai-systems"><i class="fa fa-check"></i><b>3.4</b> ATT&amp;CK for AI systems</a></li>
<li class="chapter" data-level="3.5" data-path="mitre-attck.html"><a href="mitre-attck.html#pros-and-cons"><i class="fa fa-check"></i><b>3.5</b> Pros and cons</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html"><i class="fa fa-check"></i><b>4</b> Open Worldwide Application Security Project (OWASP)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml012023-input-manipulation-attack"><i class="fa fa-check"></i><b>4.1</b> ML01:2023 Input Manipulation Attack</a></li>
<li class="chapter" data-level="4.2" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml022023-data-poisoning-attack"><i class="fa fa-check"></i><b>4.2</b> ML02:2023 Data Poisoning Attack</a></li>
<li class="chapter" data-level="4.3" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml032023-model-inversion-attack"><i class="fa fa-check"></i><b>4.3</b> ML03:2023 Model Inversion Attack</a></li>
<li class="chapter" data-level="4.4" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml042023-membership-inference-attack"><i class="fa fa-check"></i><b>4.4</b> ML04:2023 Membership Inference Attack</a></li>
<li class="chapter" data-level="4.5" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml052023-model-stealing"><i class="fa fa-check"></i><b>4.5</b> ML05:2023 Model Stealing</a></li>
<li class="chapter" data-level="4.6" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml062023-ai-supply-chain-attacks"><i class="fa fa-check"></i><b>4.6</b> ML06:2023 AI Supply Chain Attacks</a></li>
<li class="chapter" data-level="4.7" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml072023-troyan-horse-attack"><i class="fa fa-check"></i><b>4.7</b> ML07:2023 Troyan Horse Attack</a></li>
<li class="chapter" data-level="4.8" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml082023-model-skewing"><i class="fa fa-check"></i><b>4.8</b> ML08:2023 Model Skewing</a></li>
<li class="chapter" data-level="4.9" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml092023-output-integrity-attack"><i class="fa fa-check"></i><b>4.9</b> ML09:2023 Output Integrity Attack</a></li>
<li class="chapter" data-level="4.10" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml102023-model-poisoning"><i class="fa fa-check"></i><b>4.10</b> ML10:2023 Model Poisoning</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html"><i class="fa fa-check"></i><b>5</b> NIST AI Risk Management Framework (AI RMF)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#risks"><i class="fa fa-check"></i><b>5.1</b> Risks</a></li>
<li class="chapter" data-level="5.2" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#characteristics-of-trustworthy-ai-systems"><i class="fa fa-check"></i><b>5.2</b> Characteristics of trustworthy AI systems</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#valid-and-reliable"><i class="fa fa-check"></i><b>5.2.1</b> Valid and Reliable</a></li>
<li class="chapter" data-level="5.2.2" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#safe"><i class="fa fa-check"></i><b>5.2.2</b> Safe</a></li>
<li class="chapter" data-level="5.2.3" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#accountable-and-transparent"><i class="fa fa-check"></i><b>5.2.3</b> Accountable and Transparent</a></li>
<li class="chapter" data-level="5.2.4" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#explainable-and-interpretable"><i class="fa fa-check"></i><b>5.2.4</b> Explainable and Interpretable</a></li>
<li class="chapter" data-level="5.2.5" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#privacy-enhanced"><i class="fa fa-check"></i><b>5.2.5</b> Privacy-Enhanced</a></li>
<li class="chapter" data-level="5.2.6" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#fair-with-harmful-bias-managed"><i class="fa fa-check"></i><b>5.2.6</b> Fair – with Harmful Bias Managed</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#ai-rmf-core"><i class="fa fa-check"></i><b>5.3</b> AI RMF Core</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#functions"><i class="fa fa-check"></i><b>5.3.1</b> Functions</a></li>
<li class="chapter" data-level="5.3.2" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#cartegories"><i class="fa fa-check"></i><b>5.3.2</b> Cartegories</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#adversarial-machine-learning"><i class="fa fa-check"></i><b>5.4</b> Adversarial Machine Learning</a></li>
<li class="chapter" data-level="5.5" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#other-relevant-nist-initiatives"><i class="fa fa-check"></i><b>5.5</b> Other relevant NIST initiatives</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="adversarial-robustness-toolbox-art.html"><a href="adversarial-robustness-toolbox-art.html"><i class="fa fa-check"></i><b>6</b> Adversarial Robustness Toolbox (ART)</a></li>
<li class="chapter" data-level="7" data-path="dependable-and-explainable-learning-deel.html"><a href="dependable-and-explainable-learning-deel.html"><i class="fa fa-check"></i><b>7</b> DEpendable and Explainable Learning (DEEL)</a></li>
<li class="chapter" data-level="8" data-path="failure-mode-and-effects-analysis-fmea.html"><a href="failure-mode-and-effects-analysis-fmea.html"><i class="fa fa-check"></i><b>8</b> Failure Mode and Effects Analysis (FMEA)</a></li>
<li class="chapter" data-level="9" data-path="iso-27005.html"><a href="iso-27005.html"><i class="fa fa-check"></i><b>9</b> ISO 27005</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Adversarial Model Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="nist-ai-risk-management-framework-ai-rmf" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> NIST AI Risk Management Framework (AI RMF)<a href="nist-ai-risk-management-framework-ai-rmf.html#nist-ai-risk-management-framework-ai-rmf" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><a href="https://nist.gov/">The National Institute of Standards and Technology (NIST)</a> was founded in 1901 and is now part of the U.S. Department of Commerce. NIST mission is to promote U.S. innovation and industrial competitiveness by advancing measurement science, standards, and technology in ways that enhance economic security and improve our quality of life.</p>
<p>In collaboration with the private and public sectors, NIST has developed a framework to better manage risks to individuals, organizations, and society associated with artificial intelligence (AI). The NIST AI Risk Management Framework (AI RMF)
it is designed for voluntary adoption, aiming to enhance trustworthiness of AI solutions. First version of this framework was released on January 26, 2023. Two months later, on March 30, NIST launched the <a href="https://airc.nist.gov/Home">Trustworthy and Responsible AI Resource Center</a>, which will facilitate implementation of, and international alignment with, the AI RMF.</p>
<p>A key starting point for the NIST RMF <span class="citation"><a href="#ref-rmf_nist_2023">[6]</a></span> is the observation that the risks associated with systems using ML and AI components are different from those for classical IT systems. Examples of these differences are numerous, such as the adaptive nature of ML modules that adjust to training data that may change over time, the black box nature of many ML/AI systems, the multiplicity of applications for ML/AI modules that may have been trained on non-representative data. For this reason, although there are many other tools for managing risks in AI systems, a new standard for managing risks in AI-based systems is needed.</p>
<div id="risks" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Risks<a href="nist-ai-risk-management-framework-ai-rmf.html#risks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the context of the AI RMF, risk refers to <em>the composite measure of an event’s probability of occurring and the magnitude or degree of the consequences of the corresponding event</em>.</p>
<p>AI risk management offers a path to minimize potential negative impacts of AI systems,
such as threats to civil liberties and rights, while also providing opportunities to maximize positive impacts. Addressing, documenting, and managing AI risks and potential negative impacts effectively can lead to more trustworthy AI systems.</p>
<div class="float">
<img src="images/nist_harms.png" alt="Potential harms related to AI systems. It is Figure 1 in NIST RMF" />
<div class="figcaption">Potential harms related to AI systems. It is Figure 1 in NIST RMF</div>
</div>
</div>
<div id="characteristics-of-trustworthy-ai-systems" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Characteristics of trustworthy AI systems<a href="nist-ai-risk-management-framework-ai-rmf.html#characteristics-of-trustworthy-ai-systems" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A comprehensive approach to risk management calls for balancing tradeoffs
among the trustworthiness characteristics.</p>
<div class="float">
<img src="images/nist_characteristics.png" alt="Characteristics of trustworthy AI systems. It is Figure 4 in NIST RMF" />
<div class="figcaption">Characteristics of trustworthy AI systems. It is Figure 4 in NIST RMF</div>
</div>
<div id="valid-and-reliable" class="section level3 hasAnchor" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Valid and Reliable<a href="nist-ai-risk-management-framework-ai-rmf.html#valid-and-reliable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Validation</strong> is the “confirmation, through the provision of objective evidence, that the requirements for a specific intended use or application have been fulfilled” (ISO
9000:2015)</p>
<p><strong>Reliability</strong> is defined in the same standard as the “ability of an item to perform as required, without failure, for a given time interval, under given conditions” (ISO/IEC TS
5723:2022).</p>
<p><strong>Accuracy</strong> is the “closeness of results of observations, computations, or estimates to the true values or the values accepted as being true.” (ISO/IEC TS 5723:2022)</p>
<p><strong>Robustness or generalizability</strong> is defined as the “ability of a system to maintain its level of performance under a variety of circumstances” (ISO/IEC TS 5723:2022)</p>
</div>
<div id="safe" class="section level3 hasAnchor" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Safe<a href="nist-ai-risk-management-framework-ai-rmf.html#safe" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Safe</strong> AI systems should “not under defined conditions, lead to a state in which human life, health, property, or the environment is endangered” (ISO/IEC TS 5723:2022).</p>
<p><strong>Resilient</strong> system can withstand unexpected adverse events or unexpected changes in their environment or use – or if they can maintain their functions and structure in the face of internal and external change and degrade safely and gracefully when this is necessary (ISO/IEC TS 5723:2022).</p>
<p><strong>Secure</strong> AI system can maintain confidentiality, integrity, and availability through protection mechanisms that prevent unauthorized access and us.</p>
</div>
<div id="accountable-and-transparent" class="section level3 hasAnchor" number="5.2.3">
<h3><span class="header-section-number">5.2.3</span> Accountable and Transparent<a href="nist-ai-risk-management-framework-ai-rmf.html#accountable-and-transparent" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Transparency</strong> reflects the extent to which information about an AI system and its outputs is available to individuals interacting with such a system – regardless of whether they are even aware that they are doing so.</p>
<p><strong>Accountability</strong> presupposes transparency.</p>
</div>
<div id="explainable-and-interpretable" class="section level3 hasAnchor" number="5.2.4">
<h3><span class="header-section-number">5.2.4</span> Explainable and Interpretable<a href="nist-ai-risk-management-framework-ai-rmf.html#explainable-and-interpretable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Explainability</strong> refers to a representation of the mechanisms underlying AI systems’ operation, whereas <strong>interpretability</strong> refers to the meaning of AI systems’ output in the context of their designed functional purposes.</p>
<p>Transparency, explainability, and interpretability are distinct characteristics that support each other. <strong>Transparency</strong> can answer the question of “what happened” in the system. <strong>Explainability</strong> can answer the question of “how” a decision was made in the system. <strong>Interpretability</strong> can answer the question of “why” a decision was made by the system and its meaning or context to the user.</p>
</div>
<div id="privacy-enhanced" class="section level3 hasAnchor" number="5.2.5">
<h3><span class="header-section-number">5.2.5</span> Privacy-Enhanced<a href="nist-ai-risk-management-framework-ai-rmf.html#privacy-enhanced" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Privacy</strong> refers to the norms and practices that help to safeguard human autonomy, identity, and dignity. These norms and practices typically address freedom from intrusion, limiting observation, or individuals’ agency to consent to disclosure or control of facets of their identities (e.g., body, data, reputation).</p>
</div>
<div id="fair-with-harmful-bias-managed" class="section level3 hasAnchor" number="5.2.6">
<h3><span class="header-section-number">5.2.6</span> Fair – with Harmful Bias Managed<a href="nist-ai-risk-management-framework-ai-rmf.html#fair-with-harmful-bias-managed" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Fairness</strong> in AI includes concerns for equality and equity by addressing issues such as harmful bias and discrimination. Standards of fairness can be complex and difficult to define because perceptions of fairness differ among cultures and may shift depending on application.</p>
<p><strong>Bias</strong> is broader than demographic balance and data representativeness. NIST has identified three major categories of AI bias to be considered and managed: systemic, computational and statistical, and human-cognitive.</p>
</div>
</div>
<div id="ai-rmf-core" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> AI RMF Core<a href="nist-ai-risk-management-framework-ai-rmf.html#ai-rmf-core" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="functions" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Functions<a href="nist-ai-risk-management-framework-ai-rmf.html#functions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The actions that can be taken in risk management are grouped into categories and subcategories. These in turn are grouped into four overarching functions in risk management, these are Govern, Map, Measure and Manage.</p>
<p>Risk management is an ongoing process that requires taking into account different perspectives. These four functions correspond to the main perspectives that need to be considered throughout the life cycle of an AI system.</p>
<div class="float">
<img src="images/nist_functions.png" alt="The four main functions defined by the NIST framework related to AI-risks." />
<div class="figcaption">The four main functions defined by the NIST framework related to AI-risks.</div>
</div>
</div>
<div id="cartegories" class="section level3 hasAnchor" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Cartegories<a href="nist-ai-risk-management-framework-ai-rmf.html#cartegories" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Cartegories, subcartegories, outcomes and actions are listed in <span class="citation"><a href="#ref-rmf_nist_2023">[6]</a></span>
<a href="https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf#page=29.65" class="uri">https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf#page=29.65</a></p>
<div class="float">
<img src="images/nist_categories.png" alt="Main categories for each of the four NIST functions to help organizations address the risks of AI systems in practice." />
<div class="figcaption">Main categories for each of the four NIST functions to help organizations address the risks of AI systems in practice.</div>
</div>
</div>
</div>
<div id="adversarial-machine-learning" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Adversarial Machine Learning<a href="nist-ai-risk-management-framework-ai-rmf.html#adversarial-machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A good companion to the RMF Framework is another NIST document ,,Adversarial Machine Learning. A Taxonomy and Terminology of Attacks and Mitigations’’ <span class="citation"><a href="#ref-vassilev_adversarial_2024">[7]</a></span>.
It presents an up-to-date map of available attacks on predictive models with a particular focus (unlike OWASP and MITTRE) on the model perspective. The standard includes detailed explanations of each type of attack with examples. A concise summary of the attacks discussed is provided in the figure below.</p>
<div class="float">
<img src="images/NIST_attacks.png" alt="Reconstruction of Figure 1 from the ,,Taxonomy of attacks on Predictive AI systems’’ [7]" />
<div class="figcaption">Reconstruction of Figure 1 from the ,,Taxonomy of attacks on Predictive AI systems’’ <span class="citation"><a href="#ref-vassilev_adversarial_2024">[7]</a></span></div>
</div>
</div>
<div id="other-relevant-nist-initiatives" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Other relevant NIST initiatives<a href="nist-ai-risk-management-framework-ai-rmf.html#other-relevant-nist-initiatives" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>NIST has also produced a number of other documents and standards of interest from a model analysis perspective.</p>
<ul>
<li>Reducing Risks Posed by Synthetic Content. An Overview of Technical Approaches to Digital Content Transparency. <span class="citation"><a href="#ref-nist-synthetic-content">[8]</a></span>. a broad discussion of systems for generative AI and, in particular, techniques for detecting the provenance of text, image, audio and video content.</li>
</ul>
<div class="float">
<img src="images/NIST_IR_8269.png" alt="Figure 2 from draft of [8]" />
<div class="figcaption">Figure 2 from draft of <span class="citation"><a href="#ref-nist-synthetic-content">[8]</a></span></div>
</div>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body">
<div id="ref-rmf_nist_2023" class="csl-entry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">NIST, <span>“<span>Artificial Intelligence Risk Management Framework (AI RMF)</span>.”</span> NIST, 2023. Accessed: Jan. 2023. [Online]. Available: <a href="https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf">https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf</a></div>
</div>
<div id="ref-vassilev_adversarial_2024" class="csl-entry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">A. Vassilev, A. Oprea, A. Fordyce, and H. Anderson, <span>“Adversarial machine learning: A taxonomy and terminology of attacks and mitigations,”</span> National Institute of Standards; Technology (U.S.), Gaithersburg, MD, NIST 100-2e2023, 2024. doi: <a href="https://doi.org/10.6028/NIST.AI.100-2e2023">10.6028/NIST.AI.100-2e2023</a>.</div>
</div>
<div id="ref-nist-synthetic-content" class="csl-entry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">NIST, <em><span class="nocase">Reducing Risks Posed by Synthetic Content. An Overview of Technical Approaches to Digital Content Transparency</span></em>. NIST, 2024. Available: <a href="https://airc.nist.gov/docs/NIST.AI.100-4.SyntheticContent.ipd.pdf">https://airc.nist.gov/docs/NIST.AI.100-4.SyntheticContent.ipd.pdf</a></div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="open-worldwide-application-security-project-owasp.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="adversarial-robustness-toolbox-art.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/pbiecek/ama/edit/master/30-nist.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
