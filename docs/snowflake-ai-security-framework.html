<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Snowflake AI security framework | Adversarial Model Analysis</title>
  <meta name="description" content="Red Team Notes" />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Snowflake AI security framework | Adversarial Model Analysis" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/images/cover.png" />
  <meta property="og:description" content="Red Team Notes" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Snowflake AI security framework | Adversarial Model Analysis" />
  
  <meta name="twitter:description" content="Red Team Notes" />
  <meta name="twitter:image" content="/images/cover.png" />

<meta name="author" content="Przemysław Biecek" />


<meta name="date" content="2024-05-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="adversarial-robustness-toolbox-art.html"/>
<link rel="next" href="dependable-and-explainable-learning-deel.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><h3>Adversarial Model Analysis</h3> RedTeam Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html"><i class="fa fa-check"></i><b>1</b> The Two XAI Cultures</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#motivation"><i class="fa fa-check"></i><b>1.1</b> Motivation</a></li>
<li class="chapter" data-level="1.2" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#introduction-1"><i class="fa fa-check"></i><b>1.2</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#user-oriented-explanations"><i class="fa fa-check"></i><b>1.2.1</b> User Oriented Explanations</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#the-developer-oriented-explanations"><i class="fa fa-check"></i><b>1.2.2</b> The Developer Oriented Explanations</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#challenges"><i class="fa fa-check"></i><b>1.3</b> Challenges</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#achilles-heels"><i class="fa fa-check"></i><b>1.3.1</b> Achilles’ heels</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#rashomon-perspectives"><i class="fa fa-check"></i><b>1.3.2</b> Rashomon perspectives</a></li>
<li class="chapter" data-level="1.3.3" data-path="the-two-xai-cultures.html"><a href="the-two-xai-cultures.html#champion-challenger-analysis"><i class="fa fa-check"></i><b>1.3.3</b> Champion Challenger analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="model-evaluation-levels.html"><a href="model-evaluation-levels.html"><i class="fa fa-check"></i><b>2</b> Model Evaluation Levels</a></li>
<li class="chapter" data-level="3" data-path="mitre-attck.html"><a href="mitre-attck.html"><i class="fa fa-check"></i><b>3</b> MITRE ATT&amp;CK</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mitre-attck.html"><a href="mitre-attck.html#tactics"><i class="fa fa-check"></i><b>3.1</b> Tactics</a></li>
<li class="chapter" data-level="3.2" data-path="mitre-attck.html"><a href="mitre-attck.html#techniques"><i class="fa fa-check"></i><b>3.2</b> Techniques</a></li>
<li class="chapter" data-level="3.3" data-path="mitre-attck.html"><a href="mitre-attck.html#procedures"><i class="fa fa-check"></i><b>3.3</b> Procedures</a></li>
<li class="chapter" data-level="3.4" data-path="mitre-attck.html"><a href="mitre-attck.html#attck-for-ai-systems"><i class="fa fa-check"></i><b>3.4</b> ATT&amp;CK for AI systems</a></li>
<li class="chapter" data-level="3.5" data-path="mitre-attck.html"><a href="mitre-attck.html#pros-and-cons"><i class="fa fa-check"></i><b>3.5</b> Pros and cons</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html"><i class="fa fa-check"></i><b>4</b> Open Worldwide Application Security Project (OWASP)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml012023-input-manipulation-attack"><i class="fa fa-check"></i><b>4.1</b> ML01:2023 Input Manipulation Attack</a></li>
<li class="chapter" data-level="4.2" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml022023-data-poisoning-attack"><i class="fa fa-check"></i><b>4.2</b> ML02:2023 Data Poisoning Attack</a></li>
<li class="chapter" data-level="4.3" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml032023-model-inversion-attack"><i class="fa fa-check"></i><b>4.3</b> ML03:2023 Model Inversion Attack</a></li>
<li class="chapter" data-level="4.4" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml042023-membership-inference-attack"><i class="fa fa-check"></i><b>4.4</b> ML04:2023 Membership Inference Attack</a></li>
<li class="chapter" data-level="4.5" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml052023-model-stealing"><i class="fa fa-check"></i><b>4.5</b> ML05:2023 Model Stealing</a></li>
<li class="chapter" data-level="4.6" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml062023-ai-supply-chain-attacks"><i class="fa fa-check"></i><b>4.6</b> ML06:2023 AI Supply Chain Attacks</a></li>
<li class="chapter" data-level="4.7" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml072023-troyan-horse-attack"><i class="fa fa-check"></i><b>4.7</b> ML07:2023 Troyan Horse Attack</a></li>
<li class="chapter" data-level="4.8" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml082023-model-skewing"><i class="fa fa-check"></i><b>4.8</b> ML08:2023 Model Skewing</a></li>
<li class="chapter" data-level="4.9" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml092023-output-integrity-attack"><i class="fa fa-check"></i><b>4.9</b> ML09:2023 Output Integrity Attack</a></li>
<li class="chapter" data-level="4.10" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml102023-model-poisoning"><i class="fa fa-check"></i><b>4.10</b> ML10:2023 Model Poisoning</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html"><i class="fa fa-check"></i><b>5</b> NIST AI Risk Management Framework (AI RMF)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#risks"><i class="fa fa-check"></i><b>5.1</b> Risks</a></li>
<li class="chapter" data-level="5.2" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#characteristics-of-trustworthy-ai-systems"><i class="fa fa-check"></i><b>5.2</b> Characteristics of trustworthy AI systems</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#valid-and-reliable"><i class="fa fa-check"></i><b>5.2.1</b> Valid and Reliable</a></li>
<li class="chapter" data-level="5.2.2" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#safe"><i class="fa fa-check"></i><b>5.2.2</b> Safe</a></li>
<li class="chapter" data-level="5.2.3" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#accountable-and-transparent"><i class="fa fa-check"></i><b>5.2.3</b> Accountable and Transparent</a></li>
<li class="chapter" data-level="5.2.4" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#explainable-and-interpretable"><i class="fa fa-check"></i><b>5.2.4</b> Explainable and Interpretable</a></li>
<li class="chapter" data-level="5.2.5" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#privacy-enhanced"><i class="fa fa-check"></i><b>5.2.5</b> Privacy-Enhanced</a></li>
<li class="chapter" data-level="5.2.6" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#fair-with-harmful-bias-managed"><i class="fa fa-check"></i><b>5.2.6</b> Fair – with Harmful Bias Managed</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#ai-rmf-core"><i class="fa fa-check"></i><b>5.3</b> AI RMF Core</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#functions"><i class="fa fa-check"></i><b>5.3.1</b> Functions</a></li>
<li class="chapter" data-level="5.3.2" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#cartegories"><i class="fa fa-check"></i><b>5.3.2</b> Cartegories</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#adversarial-machine-learning"><i class="fa fa-check"></i><b>5.4</b> Adversarial Machine Learning</a></li>
<li class="chapter" data-level="5.5" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html#other-relevant-nist-initiatives"><i class="fa fa-check"></i><b>5.5</b> Other relevant NIST initiatives</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="adversarial-robustness-toolbox-art.html"><a href="adversarial-robustness-toolbox-art.html"><i class="fa fa-check"></i><b>6</b> Adversarial Robustness Toolbox (ART)</a></li>
<li class="chapter" data-level="7" data-path="snowflake-ai-security-framework.html"><a href="snowflake-ai-security-framework.html"><i class="fa fa-check"></i><b>7</b> Snowflake AI security framework</a></li>
<li class="chapter" data-level="8" data-path="dependable-and-explainable-learning-deel.html"><a href="dependable-and-explainable-learning-deel.html"><i class="fa fa-check"></i><b>8</b> DEpendable and Explainable Learning (DEEL)</a></li>
<li class="chapter" data-level="9" data-path="failure-mode-and-effects-analysis-fmea.html"><a href="failure-mode-and-effects-analysis-fmea.html"><i class="fa fa-check"></i><b>9</b> Failure Mode and Effects Analysis (FMEA)</a></li>
<li class="chapter" data-level="10" data-path="iso-27005.html"><a href="iso-27005.html"><i class="fa fa-check"></i><b>10</b> ISO 27005</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Adversarial Model Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="snowflake-ai-security-framework" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Snowflake AI security framework<a href="snowflake-ai-security-framework.html#snowflake-ai-security-framework" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>AI security framework is a whitepaper published by Snowflake in 2024. This whitepaper approaches the security problem from the perspective of potential threats. For each of the 19 threats, there is a structured presentation of the potential (negative) effect when an adversary exploits the threat, examples of attacks, potential mitigation strategies (only a selection are presented below), what is being attacked and what risks are involved.</p>
<p>The following table summarises the threats and risks listed in the paper <span class="citation"><a href="#ref-aisf-snowflake">[6]</a></span>.</p>
<table>
<colgroup>
<col width="8%" />
<col width="44%" />
<col width="2%" />
<col width="11%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th>Threat</th>
<th>Short info</th>
<th>Category</th>
<th>Main Risk</th>
<th>Mitigation Strategies</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Training Data Leakage</td>
<td>exposure of sensitive information that may be contained within the dataset(s) used to train a model</td>
<td>Model, Data</td>
<td>Confidentiality, Compliance, Legal</td>
<td>Data Anonymization; Train-Test Split; Access Controls; Secure Data Transmission</td>
</tr>
<tr class="even">
<td>Privacy</td>
<td>privacy concerns, relating to the collection, storage, and utilization of sensitive data to train and deploy models</td>
<td>Model, Data</td>
<td>Confidentiality, Compliance, Legal</td>
<td>Differential Privacy; Data Minimization; Secure Multiparty Computation (SMC); Federated Learning;  Privacy-Preserving Evaluation</td>
</tr>
<tr class="odd">
<td>Bias</td>
<td>presence of systematic and unfair inaccuracies in predictions made by the models</td>
<td>Model, Data</td>
<td>Integrity, Compliance, Legal</td>
<td>Data collection and cleansing; Algorithm selection and design; Human oversight and auditing; Continuous monitoring and feedback; FairGAN</td>
</tr>
<tr class="even">
<td>Lack of Explainability / Transparency</td>
<td>opacity, often referred to as the “black box” problem, presents a significant challenge across various domains, …, explainability is crucial for building trust, mitigating bias and ensuring responsible AI development</td>
<td>Model, Data</td>
<td>Integrity, Compliance, Legal</td>
<td>Feature Importance; Model Explainability; Counterfactual Explanations; Adversarial Training; Model Transparency and Documentation</td>
</tr>
<tr class="odd">
<td>Backdooring Models (Insider Attacks)</td>
<td>manipulate models during training to introduce hidden vulnerabilities, allowing them to trigger malicious behavior during deployment</td>
<td>Model, Data</td>
<td>Integrity, Compliance, Legal</td>
<td>Secure Development Practices; Data Provenance and Auditability; Model Verification and Validation; Access Control and Monitoring; Adversary Detection and Response</td>
</tr>
<tr class="even">
<td>Prompt Injection</td>
<td>steer the model towards generating specific outputs, like hate speech, misinformation, or offensive content; constructing prompts that exploit the model’s internal biases; influence the model’s behavior beyond the intended task</td>
<td>Model</td>
<td>Integrity, Compliance, Legal</td>
<td>Prompt Validation and Filtering; Adversarial Training; Context-Aware Processing; Explainable AI Techniques; Human-in-the-Loop Systems</td>
</tr>
<tr class="odd">
<td>Indirect Prompt Injection</td>
<td>inject harmful instructions into external content, causing LLMs to generate unexpected and potentially harmful responses</td>
<td>Model</td>
<td>Integrity, Compliance, Legal</td>
<td>Sources Validation and Filtering; Explainable AI Techniques; Human-in-the-Loop Systems</td>
</tr>
<tr class="even">
<td>Adversarial Samples</td>
<td>modification of input, often imperceptible to humans, designed to mislead an AI model</td>
<td>Model</td>
<td>Integrity, Compliance, Legal</td>
<td>Robust Model Training; Defensive Distillation; Input Preprocessing; Adversarial Detection Mechanisms</td>
</tr>
<tr class="odd">
<td>Sponge Samples</td>
<td>maliciously crafted inputs designed to significantly increase energy consumption and computational latency, …, named for their ability to absorb resources</td>
<td>Model</td>
<td>Availability, Compliance, Legal</td>
<td>Input Validation and Normalization; Resource Monitoring and Throttling; Adversarial Training; Hardware-Level Optimizations</td>
</tr>
<tr class="even">
<td>Model  Stealing</td>
<td>extracting and replicating the functionality of a target model</td>
<td>Model</td>
<td>Compliance, Legal</td>
<td>Secure Model Deployment; Watermarking and Fingerprinting; Differential Privacy Techniques; Access Control Measures</td>
</tr>
<tr class="odd">
<td>Fuzzing</td>
<td>feeding the model with a deluge of diverse, often malformed, inputs, aiming to elicit unexpected behaviors or trigger crashes, such as: incorrect predictions, potentially impacting safety-critical systems, privacy violations, adversarial attacks, resource exhaustion</td>
<td>Model</td>
<td>Integrity, Availability, Compliance</td>
<td>Input Validation; Adversarial Training; Model Explainability; Fuzzing as a Defense</td>
</tr>
<tr class="even">
<td>Model Inversion</td>
<td>reverse-engineer a trained model to extract sensitive information about its training data</td>
<td>Model</td>
<td>Confidentiality, Legal</td>
<td>Limit access to the model and its predictions; Validate input data to prevent malicious inputs; Log all inputs and outputs for auditing; Compare predictions to ground truth data</td>
</tr>
<tr class="odd">
<td>Distributed Denial of Service on ML Model</td>
<td>overwhelm ML systems, exhaust their computational resources, and render them unusable</td>
<td>Model</td>
<td>Availability, Compliance, Legal</td>
<td>Capacity Planning and Scaling; Resilient Infrastructure; Filtering and Traffic Anomaly Detection; Rate Limiting and Prioritization; Adaptive Defense</td>
</tr>
<tr class="even">
<td>Model Poisoning</td>
<td>infiltrate malicious data into a model’s training process, manipulating its learning and causing significant performance degradation or biased outputs</td>
<td>Model</td>
<td>Integrity, Availability, Compliance</td>
<td>Regularly monitor the quality and integrity of training data to detect any anomalies or malicious injections</td>
</tr>
<tr class="odd">
<td>Training Data Poisoning</td>
<td>malicious manipulation of training data to compromise the integrity and performance of AI models, induce specific behaviors in the model, to induce biases, skew model predictions, or cause targeted misclassifications</td>
<td>Data</td>
<td>Integrity, Compliance, Legal</td>
<td>Data Sanitization; Robust Model Training; Input Validation; Data Sanitization; Input Validation</td>
</tr>
<tr class="even">
<td>Multitenancy in ML Enviroments</td>
<td>maliciously access sensitive information belonging to other tenants, potentially compromising confidentiality and violating trust; impact model performance and system responsiveness across tenants</td>
<td>Infrastructure</td>
<td>Confidentiality, Compliance, Legal</td>
<td>Data Encryption; Federated Learning; Differential Privacy; Secure Multi-Party Computation (SMPC); Resource Management and Monitoring</td>
</tr>
<tr class="odd">
<td>Exposure of sensitive inferential inputs</td>
<td>service, if not properly secured, might be a target to eavesdropping</td>
<td>Infrastructure</td>
<td>Confidentiality, Integrity, Legal</td>
<td>Proper configuration of encryption in transit; Implementing authentication mechanisms; Secure communication channels</td>
</tr>
<tr class="even">
<td>Attacks on the Infrastructure Hosting ML Services</td>
<td>infrastructure components may be susceptible to a range of technical attack vectors, potentially jeopardizing the entire ML system and exposing sensitive information</td>
<td>Infrastructure</td>
<td>Confidentiality, Integrity, Availability, Compliance, Legal</td>
<td>Patch Management and Vulnerability Scanning; Least Privilege and Access Control; Data Encryption; Security Testing and Penetration Testing; Continuous Monitoring and Logging; Security Awareness Training</td>
</tr>
<tr class="odd">
<td>Self-Hosted OSS LLMS Security</td>
<td>hosting and utilizing third-party, pretrained models poses security challenges</td>
<td>General</td>
<td>Confidentiality, Integrity, Availability, Compliance, Legal</td>
<td>Supply Chain Security; Cryptographic Signing; Model Testing; Malware Scanning; Secure Deployment and Monitoring</td>
</tr>
</tbody>
</table>

</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body">
<div id="ref-aisf-snowflake" class="csl-entry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">Snowflake, <em><span>AI SECURITY FRAMEWORK</span></em>. Snowflake, 2024. Available: <a href="https://www.snowflake.com/en/resources/white-paper/snowflake-ai-security-framework/">https://www.snowflake.com/en/resources/white-paper/snowflake-ai-security-framework/</a></div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="adversarial-robustness-toolbox-art.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="dependable-and-explainable-learning-deel.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/pbiecek/ama/edit/master/40-snowflake.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
