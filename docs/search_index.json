[["index.html", "Adversarial Model Analysis Introduction Acknowledgments", " Adversarial Model Analysis Przemysław Biecek 2023-12-07 Introduction The explosion of interest in machine learning coupled with the increasing availability of large amounts of data and tools for simple development of models has quickly led to an avalanche of examples of hurtful, unsafe or unfair decisions made by automated systems. The response to examples of decisions hurting citizens is a growing number of regulations. The AI act [1] discussed for several years in the European Union, President Biden’s recently issued Executive Order [2] are just two examples of the growing number of recommendations reiterating over and over that safe models must be created. And with lines of companies eager to sell audits of AI systems (arguably more lucrative than financial audits), and a crowd of lawyers trying to embrace the nomenclature used in the emerging regulations on the table, an open question remains - what technical tools do we have available to develop secure AI systems. After all, it’s not that data scientists (the sexiest job of the 21st Century) are maliciously creating poorly performing models. Nor is it the case that data-collecting companies are maliciously sewing biases into the data. Nor is it that model-training tools bury unwanted artifacts in models. The truth is that for the past decades we have been focused on developing model building tools virtually ignoring the need for in-depth verification. In this book, you will find examples of techniques that can help to create safe models. However, developing safety of a model requires a completely new mind-set. Instead of chasing performance metrics by using the strangest tricks to bump up performance on the fifth significant digit this time, one must learn to look for weaknesses in models. Spot them and fix it and iterate until no weaknesses can be found. The area of model analysis is a new topic that will rapidly expand in the years to come. Certainly many techniques not described in this book are already or will be created. If you know such techniques then please contact me, I will try to add them to this living book. Acknowledgments This book is a continuation of the Explanatory Model Analysis (EMA) book [3]. In EMA, we showed how to apply XAI techniques to better understand how the model works. In this book, we show how to use the available techniques to find potentially problematic situations where the model is not working. Cover and chapter graphics created with MidJourney. References "],["the-two-xai-cultures.html", "Chapter 1 The Two XAI Cultures 1.1 Introduction 1.2 Challenges", " Chapter 1 The Two XAI Cultures In 2001, Leo Breiman published a phenomenal paper “The Two Cultures” [4], in which he diagnosed the growing gap between the mainstream trends in academic research conducted by statisticians against new challenges, such as the curse of dimensionality, the construction of effective predictive algorithms. His paper is so well constructed that its narrative structure can be applied to diagnose problems in many research areas standing on the edge of an identity crisis. In this chapter, I argue that the field of explainable artificial intelligence (XAI) is also facing such a crisis, and two cultures of researchers are evident in this area as well. Moreover, without a better understanding of the different goals facing these cultures, no further progress is possible. In the second part of this chapter, three key challenges facing one culture are presented and illustrated using a real dataset as an example. 1.1 Introduction Machine learning starts with models. Think of a model as a function \\(f:\\mathcal R^d \\rightarrow \\mathcal R\\) that calculates a numerical score based on \\(d\\)-dimensional input. In machine learning, these functions are often constructed based on training data and their internal operations are complex, often described by thousands, millions or even billions of parameters. Direct parameter-by-parameter analysis of the model is often not feasible, but reasoning about the model’s behavior is desired. At least two goals of analyzing the model can be identified. Communication of the key premises behind model predictions. The premises should be in line with user expectations and knowledge. Discovering situations in which the model does not work as expected by an operator. In such a situation, the model’s predictions may be wrong or the operator’s expectations may be wrong. There are two different approaches toward these goals: 1.1.1 User Oriented Explanations One way to think about explanations is from the perspective of a citizen who receives a decision that he/she doesn’t understand, or disagrees with, or is curious about, and asks ‘where did this decision come from?’ Whether it involves credit, or access to medical services, or restaurant recommendations is secondary. The important thing is that the person asking ‘Why’ is the user of the AI system. This perspective places the emphasis on the model’s decision received by a particular individual and assumes that explainability will help him accept or challenge that decision. User Oriented Explanations Analysis of a model in this culture is oriented toward user rights (e.g. mythical ,,right to explain’’). Discussion of the need for and desirability of explanations revolves around such concepts as trust, right to explanation, causality, fair and ethical decision-making. informativeness. A paper that has resonated and illustrates well the perspective of this culture is [5]. When: this perspective is most prevalent after model deployment. Validation: user studies (although there are now more papers saying that a user study is needed than papers that take a professional approach to the topic of user study). Estimated culture population: majority of those publishing in the XAI area. 1.1.2 The Developer Oriented Explanations Another perspective on thinking about explanations is through eyes an engineer building or testing a model, who wants the model to work in every, or almost every, case. Explanations can help diagnose and perhaps fix errors in model performance. This perspective emphasizes the model that should work well for all sorts of data. Developer Oriented Explanations Analysis of a model in this culture is oriented toward new ideas on how to improve the model. Typical questions asked by this culture are if explanations are fidel to the model, give global perspective, can be used to debug models, can be used to improve models. A very popular paper that gives this perspective is [6]. When: this perspective is most prevalent before model deployment. Although it can also be present after deployment (e.g., monitoring model performance, port-mortem analysis). Validation: Effectiveness in finding model weaknesses, effectiveness in generating new and better models. Estimated culture population: most people using XAI in a business setting 1.2 Challenges In this section I will argue that the focus in the user oriented explanations has: Led to irrelevant theory and questionable scientific conclusions (since in mose cases the target user population is not specified) Kept ML researchers from using more suitable performance measures (since measuring trust in very shady concept) Prevented ML researchers from working on exciting new problems (since it is hard to do progress without good validation scheme) The developer-focused perspective offers interesting new challenges just waiting to be solved. I offer three example problems below. In part they intersect with Breiman’s proposals (n.p. Rashomon perspective), in part they are specific to XAI techniques. 1.2.1 Achilles’ heels Identifying potential weaknesses of the model, finding its Achilles’ heel. TODO: refer to the FIFA case and https://journal.r-project.org/archive/2019/RJ-2019-036/index.html 1.2.2 Rashomon perspectives Understanding the root cause of identified problems - Rashomon perspectives. TODO: refer to https://arxiv.org/abs/2302.13356 1.2.3 Champion Challenger analysis Apply mitigation strategies and see if they worked and led to better model behavior - Champion Challenger analysis. TODO: refer / extend analysis of funnel plot from https://modeloriented.github.io/DALEXtra/reference/funnel_measure.html References "],["model-evaluation-levels.html", "Chapter 2 Model Evaluation Levels", " Chapter 2 Model Evaluation Levels AI models are being applied to more and more important decisions, thus ensuring the reliability and trustworthiness of AI models stands as a critical endeavor. Model validation—an essential step in this process—serves as the gatekeeper between well performing and faulty model. AI models are created to automate human behavior, and just as there are no flawless human decisions, there will be no flawless model decisions. Thus, the goal of validation is not to eliminate all errors, but to identify situations in which model errors can be eliminated. When evaluating a model, both the evaluation criterion and the selection of the data sample to assess the quality of the model are important. In this section, we will look at the issue of data selection. Below we describe the five levels of model evaluation. A concept that will accompany us throughout the book are strategies to identify model weaknesses that can be fixed. Typically, this means better model behavior in areas of low data density or outside data manifold. A diagram of the five levels of model evaluation is shown in Figure 1. How safe is your AI system? On what Model Evaluation Level (MEL) you test and explain your model? Model Evaluation Level 1 - for model evaluation we use the same data on which we trained the model. This is a common practice in modelling based on linear models or other models where we have strong assumptions about the mechanism generating the data and the representativeness of the sample. For linear models, evaluation can be based on \\(R^2\\) or p-values for the significance of the coefficients. Out of all the floors, this is the most optimistic evaluation of the model. In predictive applications it is unlikely to be used and the vague word ,,overfitting’’ is given as an argument. Model Evaluation Level 2 - to evaluate the model, we use a separate data set with a distribution similar to the training data. The most common scenario is to randomly divide the data into two parts training and testing. Sometimes used in more sophisticated variants such as cross-validation or an out-of-bag data for bootstrap sampling. The characteristic of this evaluation is that the distribution of training and test data is the same, which corresponds to the assumption that the reality in which the model will operate is not different from the reality in which the model was built. This assumption definitely simplifies model validation, although covid pandemic or wars show that future data can differ from past data. Model Evaluation Level 3 - to evaluate the model, disjoint data is used for training and testing. If the data contains a timestamp, the disjointness may mean using a different time range (so called out-of-time validation [7],[8]). If the data contains a geographic tag, validation may mean using a different region (out-of-region validation). Virtually any attribute can be used to define a test sample, usually this is done to test for generalizability. For data collected from different devices, out-of-device validation can be considered, out-of-dataset for a new dataset [9] and so on. This testing scheme is used by audit teams in (for example) financial institutions, especially for large-scale, high impact models (e.g., credit scoring models). Model Evaluation Level 4 - no more indulging, it is time to actively look for situations in which the model does not work. There are situations in which users of the model may want to adversarially influence the model’s decisions. For example, for an insurance pricing model, the user is tempted to manipulate certain parameters so as to evaluate the price of the premium. Parameters such as mileage, a statement about the number of drivers or being abroad may be given incorrectly by the user if he or she thinks they will significantly reduce the price of the premium. Another example is models that detect hate speech in online forums, users who want to post offensive comments will actively test scenarios in which the models do not work. In these situations, the adversary does not know the structure of the model, and can only reflect it and thus look for model vulnerabilities. This term was also popular at the Kaggle platform [10] to check similarity of training and testing datasets. Model Evaluation Level 5 - In a more pessimistic scenario, the adversary has full access to the model. Whether it has been stolen or the model is publicly available is of little relevance. Analysis at this level allows tracking decision paths, calculating and using gradients. An example is the use of the Llama 2 [11] language model, which is publicly available, in an AI system. References "],["mitre-attck.html", "Chapter 3 MITRE ATT&amp;CK 3.1 Tactics 3.2 Techniques 3.3 Procedures 3.4 ATT&amp;CK for AI systems 3.5 Pros and cons", " Chapter 3 MITRE ATT&amp;CK Inspirations for adversarial analysis of ML models can be borrowed from concepts proposed in cyber security. One such remarkable concept is the ATT&amp;CK matrix maintained by MITRE corporations. MITRE was formed in 1958 as a military think tank to advance national security. Now it is ,,applying systems thinking to national challenges in defense, cybersecurity, healthcare, homeland security, and transportation. Solving problems for a safer world’’ (a quote from their official profile). MITRE has a number of ambitious projects underway, perhaps the best known of which is the MITRE ATT&amp;CK framework https://attack.mitre.org/. The ATT&amp;CK framework [12] is a curated publicly available knowledge base of various cyber adversary tactics and techniques that can be used across the entire attack lifecycle. It takes the perspective of the adversary collecting and cataloging the various tactics, techniques, and procedures (TTPs) used in cyber intrusions, thereby offering defenders a detailed insight into potential attack methods and aiding in the development of more robust defense strategies. ATT&amp;CK Matrix for Enterprise, screenshot from https://attack.mitre.org/ First, let’s familiarise with the nomenclature used in ATT&amp;CK framework (Adversarial Tactics, Techniques, and Common Knowledge). 3.1 Tactics A tactic refers to a high-level objective or goal that an adversary aims to achieve during a cyber attack. It represents the overarching category of activities or actions employed by attackers to accomplish their malicious objectives within a network or system. There are 12 tactics defined within the ATT&amp;CK framework, each representing a distinct phase or goal in the cyber attack lifecycle (these are columns in the matrix presented in Figure fig:mitre?): Initial Access: This tactic involves the methods an attacker uses to gain initial entry into a system or network environment. Execution: Tactics related to techniques used to run malicious code or commands on a victim’s system. Persistence: Techniques employed by attackers to maintain their foothold within a compromised system or network, ensuring continued access even after initial access has been achieved. Privilege Escalation: Methods used to obtain higher levels of access or control within a system, moving from lower-privileged accounts to higher-privileged ones. Defense Evasion: Tactics focused on techniques used by attackers to avoid detection or thwart defensive measures implemented by security solutions. Credential Access: Techniques employed to obtain account credentials or authentication tokens, which can be used to gain unauthorized access to systems or resources. Discovery: Tactics related to an attacker’s efforts to gather information about a target network or system, including identifying assets, users, and configurations. Lateral Movement: Techniques used by attackers to move through a network, exploring and accessing different systems or resources beyond the initially compromised system. Collection: Tactics involving the gathering or exfiltration of data or sensitive information from the compromised environment. Exfiltration: Techniques used to transfer stolen data or information out of the victim’s network to an external location controlled by the attacker. Impact: Tactics focused on actions that cause damage, disruption, or other negative effects on the target system or organization. 3.2 Techniques While tactique address the question Why is the objective of the adversary action, the What question is addressed by techniques. More formally a technique refers to a specific method, approach, or procedure used by adversaries to accomplish a particular objective within a given tactic during a cyber attack. There is a (growing) number of techniques for each tactic. For example, the techniques listed under the “Collection” tactic represent specific methods or procedures used by attackers to gather or harvest data from targeted systems or networks during a cyber intrusion. These techniques help adversaries achieve their goal of acquiring sensitive information or valuable data from compromised environments. Some techniques listed for the tactic “Collection” include Audio Capture, Clipboard data, Screen Capture and Email collection. For each technique, the ATT&amp;CK matrix provides examples of procedures, mitigation strategies and detection techniques. See page https://attack.mitre.org/techniques/T1123/ for an description for the Audio Capture technique. 3.3 Procedures While tactiques address the question Why, techniques address the question What then procedures are examples of How. Procedures are descriptions of the steps necessary to implement the technique to perform the technique. 3.4 ATT&amp;CK for AI systems The ATT&amp;CK matrix was developed for attacks on IT infrastructure and applies to cyber security in the classical sense. However, due to its popularity, work is underway to adapt this approach to security analysis of AI systems. An example of the solution being developed by MITRE is the ATLAS matrix [13], [14]. ATLAS - Adversarial Threat Landscape for Artificial-Intelligence Systems 3.5 Pros and cons The ATT&amp;CK matrix was designed for AI systems. It can be useful as a reference for machine learning-based systems, but it’s a risky interpolation. From the perspective of AI systems, the ATT&amp;CK matrix offers some unique pros and a new perspective, but it also has some drawbacks or shortcomings. Here is a subjective selection of one and the other. The undoubted advantage is the attacker’s perspective. The matrix resembles an hacker playbook, but at the same time allows the security team to check if some security problems are missed or not. Second advantage is the standardized language, the framework provides a standardized terminology and taxonomy for cybersecurity professionals, allowing them to communicate more effectively about threats, tactics, and techniques used by adversaries. Not to be forgotten the huge community support. New threads needs to be identified quickly and the collaboration among cybersecurity professionals and organizations, allows to stay updated with evolving threats and defensive strategies. And since AI systems are parts of IT systems ATLAS and ATT&amp;CK align new AI related threads with known operating schemes. This facilitates the synchronization of the team working on the security of AI models as well as IT infrastructure With the huge advantages of the ATT&amp;CK matrix, it is worth keeping in mind the potential weaknesses. The core element of this framework is a list of 12 tactics that describe successive processes for escalating access and control over an IT system. In the case of classic systems in which the ultimate success is often to gain control on the system, this approach makes sense and is very natural. In the case of AI systems, root access to bash the main server is not the ultimate goal and the escalation of access to the model itself also has a different flow. So be careful since familiar paths and a mindset that has proven itself in IT security does not remove new risks and new techniques for attacking AI systems from the horizon. Also the ATT&amp;CK matrix provides detailed descriptions of tactics and techniques but may lack the context of specific environments or industries, requiring customization for individual organizational needs. In the case of AI systems that are often tailored to the modality being analyzed (text, image, audio) or application (finance, medicine), the devil may be in the details. ATT&amp;CK primarily focuses on cataloging known techniques used by adversaries. It might not cover novel or emerging attack methods immediately, leaving a gap in addressing new threats. And in the case of AI systems safety, which is a very young discipline, new problems and attack ideas emerge very quickly. References "],["open-worldwide-application-security-project-owasp.html", "Chapter 4 Open Worldwide Application Security Project (OWASP) 4.1 ML01:2023 Input Manipulation Attack 4.2 ML02:2023 Data Poisoning Attack 4.3 ML03:2023 Model Inversion Attack 4.4 ML04:2023 Membership Inference Attack 4.5 ML05:2023 Model Stealing 4.6 ML06:2023 AI Supply Chain Attacks 4.7 ML07:2023 Troyan Horse Attack 4.8 ML08:2023 Model Skewing 4.9 ML09:2023 Output Integrity Attack 4.10 ML10:2023 Model Poisoning", " Chapter 4 Open Worldwide Application Security Project (OWASP) The Open Worldwide Application Security Project (OWASP) is a nonprofit foundation that works to improve the security of software [15]. One of OWASP’s best-known projects is the Top 10 - a list of the ten most significant vulnerabilities. This list is updated every two to three years due also to changing threats and attack opportunities. While classically OWASP was focused on software, new in recent months is the inclusion of specific vulnerabilities and challenges to AI systems. As a result we get two new lists of top 10 vulnerabilities. OWASP Machine Learning Security Top Ten focused on the security of ML systems and OWASP Top 10 for Large Language Model Applications focused on systems using large language models (LLMs). Top 10 OWASP ML Sec Below we briefly overview the Top 10 for Machine Learning Security. Note The list presented is still available as a draft version. Some positions are quite similar and perhaps the final Top 10 will be a little different. The following list is an abridged summary. The full version can be found at https://mltop10.info/. 4.1 ML01:2023 Input Manipulation Attack Attacks involving data manipulation, including adversarial attacks. In this scenario the attacker deliberately changes the input data to alter the model’s decisions. For example, for a model that detects the presence of a person, the attack might involve developing a special stamp whose presence causes the person to go undetected. Example mitigation strategies Adversarial training One approach to defending against input manipulation attacks is to train the model on adversarial examples. Input validation Checking input data for anomalies, such as unexpected values or patterns, and rejecting input data that may be malicious. 4.2 ML02:2023 Data Poisoning Attack Attacks involving data poisoning occur when an attacker manipulates training data to cause unwanted model behavior. Example mitigation strategies Model validation Validate the model using a separate validation set that was not used during training. This can help detect any data poisoning attacks that may have affected the training data. Anomaly detection Using anomaly detection techniques to detect any unusual behavior on the network. 4.3 ML03:2023 Model Inversion Attack Attacks involving model inversion occur when an attacker reverse-engineers a model to extract information from it, such as regarding the training set (checking if any observation was in the training data). Example mitigation strategies Access control Restricting access to the model or its predictions (authentication) can prevent attackers from obtaining the information needed to reverse engineer the model. Regular monitoring Monitoring the model’s predictions for anomalies can help detect and prevent model inversion attacks (tracking the distribution of input and output data). 4.4 ML04:2023 Membership Inference Attack Attacks involving membership inference occur when it analyzes model inferencing to see what data was in the training data, if specific observation was used in the training data. Example mitigation strategies Model obfuscation Obfuscating model predictions by adding random noise or using differential privacy techniques. Regularization, such as L1 or L2 regularization, can help prevent over-fitting the model to the training data, which can reduce the model’s ability to accurately determine whether an example is included in the training data set. 4.5 ML05:2023 Model Stealing Attacks involving model stealing occur when an attacker gains access to model parameters and is able to accurately reproduce the model’s behavior on new data. Example mitigation strategies Watermarking Adding a watermark to model code and training data can make it possible to trace the source of the theft and hold the attacker accountable. Encryption Encrypting model code, training data and other sensitive information can prevent attackers from accessing and stealing the model. 4.6 ML06:2023 AI Supply Chain Attacks Attacks involving a modified AI supply chain occur when an attacker modifies or replaces a library or machine learning model that is used by the system. This can also include data associated with machine learning models. Example mitigation strategies Use package verification tools Such as PEP 476 and Secure Package Install to verify the authenticity and integrity of packages before installing them. Use secure package repositories, that enforce strict security measures and have a package verification process. 4.7 ML07:2023 Troyan Horse Attack Attacks involving transfer learning (aka Troyan horse attacks) occur when an attacker trains a model in one task and then tunes it in another task to cause it to behave in an undesirable way. Example mitigation strategies Implement model isolation can help prevent the transfer of malicious knowledge from one model to another. Using secure and trusted training datasets can help prevent the transfer of malicious knowledge from the attacker model to the target model. 4.8 ML08:2023 Model Skewing Attacks involving model skewing occur when an attacker manipulates the distribution of new training data to cause the model to behave in undesirable ways. Example mitigation strategies Verify the authenticity of feedback data digital signatures and checksums may be useful to verify that the feedback data received by the system is authentic, and discard any data that does not match the expected format. Regularly monitor model performance and compare its predictions with actual results to detect any deviations or distortions. 4.9 ML09:2023 Output Integrity Attack Attacks on Output Integrity aim to modify or manipulate the output of a machine learning model in order to alter its behavior or cause harm to the system in which it is used. Example mitigation strategies Secure communication channels between the model and the interface responsible for displaying results should be secured using secure protocols such as SSL/TLS. Input validation should be performed on the results to check for unexpected or manipulated values. 4.10 ML10:2023 Model Poisoning Attacks involving model poisoning occurs when an attacker manipulates model parameters to cause undesirable behavior. Example mitigation strategies Regularization Adding regularization techniques, such as L1 or L2 regularization, to the loss function helps prevent over-fitting and reduces the risk of model poisoning attacks. Cryptographic techniques can be used to secure model parameters and weights and prevent unauthorized access or manipulation of these parameters. References "],["nist-ai-risk-management-framework-ai-rmf.html", "Chapter 5 NIST AI Risk Management Framework (AI RMF)", " Chapter 5 NIST AI Risk Management Framework (AI RMF) AI RISK MANAGEMENT FRAMEWORK https://www.nist.gov/itl/ai-risk-management-framework Artificial Intelligence Risk Management Framework (AI RMF 1.0) https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf NIST AI RMF Playbook Trustworthy &amp; Responsible AI Resource Center https://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook Roadmap for the NIST Artificial Intelligence Risk Management Framework (AI RMF 1.0) https://www.nist.gov/itl/ai-risk-management-framework/roadmap-nist-artificial-intelligence-risk-management-framework-ai Inne podejścia do tematu analizy ryzyka Analiza ryzyka oparta na ISO 27005. Ta metodyka jest związana z normą ISO/IEC 27005, która określa wytyczne dotyczące zarządzania ryzykiem w bezpieczeństwie informacji. Opiera się na identyfikacji aktywów, zagrożeń, podatności i ocenie ryzyka w oparciu o skalę prawdopodobieństwa i skutków Analiza ryzyka oparta na NIST. NIST Risk Management Framework (RMF) to podejście opracowane przez National Institute of Standards and Technology (NIST) w celu zarządzania ryzykiem związanym z bezpieczeństwem informacji w instytucjach federalnych Stanów Zjednoczonych. RMF to kompletne, wieloetapowe podejście, które pomaga organizacjom identyfikować, oceniać i zarządzać ryzykiem w kontekście bezpieczeństwa informacji i zarządzania informacją. Główne cele NIST Risk Management Framework to: ustanowienie procesu zarządzania ryzykiem; RMF dostarcza struktury i wytyczne w celu stworzenia skoordynowanego i powtarzalnego procesu zarządzania ryzykiem w organizacji, skupienie na kontynuowaniu działalności; RMF pomaga organizacjom w identyfikacji i ochronie kluczowych aktywów informacyjnych, które są niezbędne do kontynuacji działalności w obliczu różnych zagrożeń, integracja z systemem zarządzania informacją; RMF integruje podejście do zarządzania ryzykiem z ogólnym systemem zarządzania informacją w organizacji, aby zapewnić spójność i skuteczność działań NIST Risk Management Framework składa się z następujących faz: Faza Kontekstualizacji. W tej fazie organizacja określa kontekst, w którym prowadzone są działania, identyfikuje cele i cele bezpieczeństwa informacji, a także określa zasoby do ochrony. Faza Oceny. W tej fazie organizacja identyfikuje zagrożenia, podatności i ocenia ryzyko związane z jej aktywami informacyjnymi. Faza Wybierania Środków Ochronnych. Na podstawie wyników oceny ryzyka organizacja wybiera odpowiednie środki ochrony i strategie zarządzania ryzykiem. Faza Wdrożenia. W tej fazie organizacja wdraża wybrane środki ochrony i podejmuje działania mające na celu zminimalizowanie ryzyka. Faza Monitorowania. Organizacja nadzoruje i analizuje skuteczność wdrożonych środków ochrony oraz ocenia nowe zagrożenia, które mogą się pojawić. Faza Reagowania. W razie potrzeby organizacja przyjmuje odpowiednie działania w odpowiedzi na nowe zagrożenia lub incydenty. NIST Risk Management Framework jest szeroko stosowany w agencjach rządowych Stanów Zjednoczonych, a ponadto używają go jako punkt odniesienia inne organizacje i instytucje, które chcą wdrożyć kompleksowe podejście do zarządzania ryzykiem związanym z bezpieczeństwem informacji. Analiza ryzyka oparta na ISO 27005. Ta metodyka jest związana z normą ISO/IEC 27005, która określa wytyczne dotyczące zarządzania ryzykiem w bezpieczeństwie informacji. Opiera się na identyfikacji aktywów, zagrożeń, podatności i ocenie ryzyka w oparciu o skalę prawdopodobieństwa i skutków. Podstawowe kroki w analizie ryzyka opartej na ISO 27005 to: Identyfikacja aktywów. Pierwszym krokiem jest określenie aktywów informacyjnych wymagających ochrony, takich jak dane, systemy, aplikacje, urządzenia. Identyfikacja zagrożeń. Następnie identyfikuje się potencjalne zagrożenia, czyli źródła ryzyka, które mogą wpłynąć na bezpieczeństwo aktywów informacyjnych. Określenie podatności. W tym etapie analizuje się słabe punkty w systemach, które mogą być wykorzystane przez zainteresowanych naruszeniem bezpieczeństwa. Ocena ryzyka. Na podstawie zidentyfikowanych zagrożeń i podatności przeprowadza się ocenę ryzyka, czyli określenie prawdopodobieństwa wystąpienia zagrożenia oraz jego wpływu na aktywa informacyjne. Zarządzanie ryzykiem. Po ocenie ryzyka organizacja podejmuje decyzje dotyczące zarządzania ryzykiem. Może to obejmować wdrożenie odpowiednich środków ochronnych, przeniesienie ryzyka na ubezpieczyciela, zaakceptowanie ryzyka lub unikanie ryzyka poprzez unikanie określonych działań. Monitorowanie i aktualizacja. Analiza ryzyka jest procesem dynamicznym, dlatego ważne jest regularne monitorowanie i aktualizacja analizy ryzyka, aby uwzględnić nowe zagrożenia i zmiany w środowisku organizacyjnym. Analiza ryzyka oparta na FAIR (Factor Analysis of Information Risk). FAIR jest metodyką, która skupia się na wykorzystaniu kwantytatywnych danych do analizy ryzyka. Pomaga organizacjom ocenić ryzyko w sposób liczbowy, biorąc pod uwagę czynniki takie jak prawdopodobieństwo wystąpienia incydentu, skutki finansowe i inne parametr Metoda FMEA (Failure Mode and Effects Analysis). Jest techniką wywodzącą się z inżynierii, ale jest również stosowana w bezpieczeństwie informacji. Polega na identyfikacji różnych sposobów wystąpienia niepowodzenia (tzw. mode of failure) oraz analizie wpływu tych niepowodzeń na funkcjonowanie organizacji. "],["adversarial-robustness-toolbox-art.html", "Chapter 6 Adversarial Robustness Toolbox (ART)", " Chapter 6 Adversarial Robustness Toolbox (ART) Adversarial Robustness Toolbox (ART) v1.16 https://github.com/Trusted-AI/adversarial-robustness-toolbox "],["dependable-and-explainable-learning-deel.html", "Chapter 7 DEpendable and Explainable Learning (DEEL)", " Chapter 7 DEpendable and Explainable Learning (DEEL) https://arxiv.org/ftp/arxiv/papers/2103/2103.10529.pdf "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
