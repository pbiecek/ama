<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Open Worldwide Application Security Project (OWASP) | Adversarial Model Analysis</title>
  <meta name="description" content="Red Team Notes" />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Open Worldwide Application Security Project (OWASP) | Adversarial Model Analysis" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/images/cover.png" />
  <meta property="og:description" content="Red Team Notes" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Open Worldwide Application Security Project (OWASP) | Adversarial Model Analysis" />
  
  <meta name="twitter:description" content="Red Team Notes" />
  <meta name="twitter:image" content="/images/cover.png" />

<meta name="author" content="Przemysław Biecek" />


<meta name="date" content="2023-11-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mitre-attck.html"/>
<link rel="next" href="nist-ai-risk-management-framework-ai-rmf.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><h3>Adversarial Model Analysis</h3> RedTeam Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="model-evaluation-levels.html"><a href="model-evaluation-levels.html"><i class="fa fa-check"></i><b>1</b> Model Evaluation Levels</a></li>
<li class="chapter" data-level="2" data-path="mitre-attck.html"><a href="mitre-attck.html"><i class="fa fa-check"></i><b>2</b> MITRE ATT&amp;CK</a>
<ul>
<li class="chapter" data-level="2.1" data-path="mitre-attck.html"><a href="mitre-attck.html#tactics"><i class="fa fa-check"></i><b>2.1</b> Tactics</a></li>
<li class="chapter" data-level="2.2" data-path="mitre-attck.html"><a href="mitre-attck.html#techniques"><i class="fa fa-check"></i><b>2.2</b> Techniques</a></li>
<li class="chapter" data-level="2.3" data-path="mitre-attck.html"><a href="mitre-attck.html#procedures"><i class="fa fa-check"></i><b>2.3</b> Procedures</a></li>
<li class="chapter" data-level="2.4" data-path="mitre-attck.html"><a href="mitre-attck.html#attck-for-ai-systems"><i class="fa fa-check"></i><b>2.4</b> ATT&amp;CK for AI systems</a></li>
<li class="chapter" data-level="2.5" data-path="mitre-attck.html"><a href="mitre-attck.html#pros-and-cons"><i class="fa fa-check"></i><b>2.5</b> Pros and cons</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html"><i class="fa fa-check"></i><b>3</b> Open Worldwide Application Security Project (OWASP)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml012023-input-manipulation-attack"><i class="fa fa-check"></i><b>3.1</b> ML01:2023 Input Manipulation Attack</a></li>
<li class="chapter" data-level="3.2" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml022023-data-poisoning-attack"><i class="fa fa-check"></i><b>3.2</b> ML02:2023 Data Poisoning Attack</a></li>
<li class="chapter" data-level="3.3" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml032023-model-inversion-attack"><i class="fa fa-check"></i><b>3.3</b> ML03:2023 Model Inversion Attack</a></li>
<li class="chapter" data-level="3.4" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml042023-membership-inference-attack"><i class="fa fa-check"></i><b>3.4</b> ML04:2023 Membership Inference Attack</a></li>
<li class="chapter" data-level="3.5" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml052023-model-stealing"><i class="fa fa-check"></i><b>3.5</b> ML05:2023 Model Stealing</a></li>
<li class="chapter" data-level="3.6" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml062023-ai-supply-chain-attacks"><i class="fa fa-check"></i><b>3.6</b> ML06:2023 AI Supply Chain Attacks</a></li>
<li class="chapter" data-level="3.7" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml072023-troyan-horse-attack"><i class="fa fa-check"></i><b>3.7</b> ML07:2023 Troyan Horse Attack</a></li>
<li class="chapter" data-level="3.8" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml082023-model-skewing"><i class="fa fa-check"></i><b>3.8</b> ML08:2023 Model Skewing</a></li>
<li class="chapter" data-level="3.9" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml092023-output-integrity-attack"><i class="fa fa-check"></i><b>3.9</b> ML09:2023 Output Integrity Attack</a></li>
<li class="chapter" data-level="3.10" data-path="open-worldwide-application-security-project-owasp.html"><a href="open-worldwide-application-security-project-owasp.html#ml102023-model-poisoning"><i class="fa fa-check"></i><b>3.10</b> ML10:2023 Model Poisoning</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="nist-ai-risk-management-framework-ai-rmf.html"><a href="nist-ai-risk-management-framework-ai-rmf.html"><i class="fa fa-check"></i><b>4</b> NIST AI Risk Management Framework (AI RMF)</a></li>
<li class="chapter" data-level="5" data-path="adversarial-robustness-toolbox-art.html"><a href="adversarial-robustness-toolbox-art.html"><i class="fa fa-check"></i><b>5</b> Adversarial Robustness Toolbox (ART)</a></li>
<li class="chapter" data-level="6" data-path="dependable-and-explainable-learning-deel.html"><a href="dependable-and-explainable-learning-deel.html"><i class="fa fa-check"></i><b>6</b> DEpendable and Explainable Learning (DEEL)</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Adversarial Model Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="open-worldwide-application-security-project-owasp" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Open Worldwide Application Security Project (OWASP)<a href="open-worldwide-application-security-project-owasp.html#open-worldwide-application-security-project-owasp" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The Open Worldwide Application Security Project (OWASP) is a nonprofit foundation that works to improve the security of software <span class="citation"><a href="#ref-owasp">[6]</a></span>.</p>
<p>One of OWASP’s best-known projects is the Top 10 - a list of the ten most significant vulnerabilities. This list is updated every two to three years due also to changing threats and attack opportunities.
While classically OWASP was focused on software, new in recent months is the inclusion of specific vulnerabilities and challenges to AI systems.
As a result we get two new lists of top 10 vulnerabilities. <a href="https://owasp.org/www-project-machine-learning-security-top-10/">OWASP Machine Learning Security Top Ten</a> focused on the security of ML systems and <a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/">OWASP Top 10 for Large Language Model Applications</a> focused on systems using large language models (LLMs).</p>
<div class="float">
<img src="images/owasp.png" alt="Top 10 OWASP ML Sec" />
<div class="figcaption">Top 10 OWASP ML Sec</div>
</div>
<p>Below we briefly overview the Top 10 for Machine Learning Security.</p>
<p><strong>Note</strong> <em>The list presented is still available as a draft version. Some positions are quite similar and perhaps the final Top 10 will be a little different. The following list is an abridged summary. The full version can be found at <a href="https://mltop10.info/" class="uri">https://mltop10.info/</a>.</em></p>
<div id="ml012023-input-manipulation-attack" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> ML01:2023 Input Manipulation Attack<a href="open-worldwide-application-security-project-owasp.html#ml012023-input-manipulation-attack" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Attacks involving data manipulation, including adversarial attacks. In this scenario the attacker deliberately changes the input data to alter the model’s decisions. For example, for a model that detects the presence of a person, the attack might involve developing a special stamp whose presence causes the person to go undetected.</p>
<p><strong>Example mitigation strategies</strong></p>
<p><em>Adversarial training</em> One approach to defending against input manipulation attacks is to train the model on adversarial examples.</p>
<p><em>Input validation</em> Checking input data for anomalies, such as unexpected values or patterns, and rejecting input data that may be malicious.</p>
</div>
<div id="ml022023-data-poisoning-attack" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> ML02:2023 Data Poisoning Attack<a href="open-worldwide-application-security-project-owasp.html#ml022023-data-poisoning-attack" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Attacks involving data poisoning occur when an attacker manipulates training data to cause unwanted model behavior.</p>
<p><strong>Example mitigation strategies</strong></p>
<p><em>Model validation</em> Validate the model using a separate validation set that was not used during training. This can help detect any data poisoning attacks that may have affected the training data.</p>
<p><em>Anomaly detection</em> Using anomaly detection techniques to detect any unusual behavior on the network.</p>
</div>
<div id="ml032023-model-inversion-attack" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> ML03:2023 Model Inversion Attack<a href="open-worldwide-application-security-project-owasp.html#ml032023-model-inversion-attack" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Attacks involving model inversion occur when an attacker reverse-engineers a model to extract information from it, such as regarding the training set (checking if any observation was in the training data).</p>
<p><strong>Example mitigation strategies</strong></p>
<p><em>Access control</em> Restricting access to the model or its predictions (authentication) can prevent attackers from obtaining the information needed to reverse engineer the model.</p>
<p><em>Regular monitoring</em> Monitoring the model’s predictions for anomalies can help detect and prevent model inversion attacks (tracking the distribution of input and output data).</p>
</div>
<div id="ml042023-membership-inference-attack" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> ML04:2023 Membership Inference Attack<a href="open-worldwide-application-security-project-owasp.html#ml042023-membership-inference-attack" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Attacks involving membership inference occur when it analyzes model inferencing to see what data was in the training data, if specific observation was used in the training data.</p>
<p><strong>Example mitigation strategies</strong></p>
<p><em>Model obfuscation</em> Obfuscating model predictions by adding random noise or using differential privacy techniques.</p>
<p><em>Regularization</em>, such as L1 or L2 regularization, can help prevent over-fitting the model to the training data, which can reduce the model’s ability to accurately determine whether an example is included in the training data set.</p>
</div>
<div id="ml052023-model-stealing" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> ML05:2023 Model Stealing<a href="open-worldwide-application-security-project-owasp.html#ml052023-model-stealing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Attacks involving model stealing occur when an attacker gains access to model parameters and is able to accurately reproduce the model’s behavior on new data.</p>
<p><strong>Example mitigation strategies</strong></p>
<p><em>Watermarking</em> Adding a watermark to model code and training data can make it possible to trace the source of the theft and hold the attacker accountable.</p>
<p><em>Encryption</em> Encrypting model code, training data and other sensitive information can prevent attackers from accessing and stealing the model.</p>
</div>
<div id="ml062023-ai-supply-chain-attacks" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> ML06:2023 AI Supply Chain Attacks<a href="open-worldwide-application-security-project-owasp.html#ml062023-ai-supply-chain-attacks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Attacks involving a modified AI supply chain occur when an attacker modifies or replaces a library or machine learning model that is used by the system. This can also include data associated with machine learning models.</p>
<p><strong>Example mitigation strategies</strong></p>
<p><em>Use package verification tools</em> Such as PEP 476 and Secure Package Install to verify the authenticity and integrity of packages before installing them.</p>
<p><em>Use secure package repositories</em>, that enforce strict security measures and have a package verification process.</p>
</div>
<div id="ml072023-troyan-horse-attack" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> ML07:2023 Troyan Horse Attack<a href="open-worldwide-application-security-project-owasp.html#ml072023-troyan-horse-attack" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Attacks involving transfer learning (aka Troyan horse attacks) occur when an attacker trains a model in one task and then tunes it in another task to cause it to behave in an undesirable way.</p>
<p><strong>Example mitigation strategies</strong></p>
<p><em>Implement model isolation</em> can help prevent the transfer of malicious knowledge from one model to another.</p>
<p><em>Using secure and trusted training datasets</em> can help prevent the transfer of malicious knowledge from the attacker model to the target model.</p>
</div>
<div id="ml082023-model-skewing" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> ML08:2023 Model Skewing<a href="open-worldwide-application-security-project-owasp.html#ml082023-model-skewing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Attacks involving model skewing occur when an attacker manipulates the distribution of new training data to cause the model to behave in undesirable ways.</p>
<p><strong>Example mitigation strategies</strong></p>
<p><em>Verify the authenticity of feedback data</em> digital signatures and checksums may be useful to verify that the feedback data received by the system is authentic, and discard any data that does not match the expected format.</p>
<p><em>Regularly monitor model performance</em> and compare its predictions with actual results to detect any deviations or distortions.</p>
</div>
<div id="ml092023-output-integrity-attack" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> ML09:2023 Output Integrity Attack<a href="open-worldwide-application-security-project-owasp.html#ml092023-output-integrity-attack" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Attacks on Output Integrity aim to modify or manipulate the output of a machine learning model in order to alter its behavior or cause harm to the system in which it is used.</p>
<p><strong>Example mitigation strategies</strong></p>
<p><em>Secure communication channels</em> between the model and the interface responsible for displaying results should be secured using secure protocols such as SSL/TLS.</p>
<p><em>Input validation</em> should be performed on the results to check for unexpected or manipulated values.</p>
</div>
<div id="ml102023-model-poisoning" class="section level2 hasAnchor" number="3.10">
<h2><span class="header-section-number">3.10</span> ML10:2023 Model Poisoning<a href="open-worldwide-application-security-project-owasp.html#ml102023-model-poisoning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Attacks involving model poisoning occurs when an attacker manipulates model parameters to cause undesirable behavior.</p>
<p><strong>Example mitigation strategies</strong></p>
<p><em>Regularization</em> Adding regularization techniques, such as L1 or L2 regularization, to the loss function helps prevent over-fitting and reduces the risk of model poisoning attacks.</p>
<p><em>Cryptographic techniques</em> can be used to secure model parameters and weights and prevent unauthorized access or manipulation of these parameters.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body">
<div id="ref-owasp" class="csl-entry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">OWASP, <em><span>Open Worldwide Application Security Project</span></em>. OWASP, 2023. Available: <a href="https://owasp.org/">https://owasp.org/</a></div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mitre-attck.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="nist-ai-risk-management-framework-ai-rmf.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/pbiecek/ama/edit/master/20-owasp.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
